{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56594960-e360-41fc-974b-a71a246b00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tldextract\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a22c8f-f22c-47b1-9493-a2d6054f958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"C:/Users/Akoba/Desktop/START up/Malicious Domain/MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ced20c-6a61-47f1-b56c-b42db428ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "alexa_path = os.path.join(base_dir, \"Data/top-1m.csv\")\n",
    "alexa_df = pd.read_csv(alexa_path, header=None, names=[\"rank\", \"domain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dabb0e-452a-467e-a19f-7af4371fdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Deduplication\n",
    "alexa_df = alexa_df.drop_duplicates(subset=\"domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d830e6c-083d-4355-804a-b1523f70ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Noise Removal\n",
    "# Remove rows where domain is not a string or is empty\n",
    "alexa_df = alexa_df[alexa_df[\"domain\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5826b6e-0446-4e02-b9bc-d284e136c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Normalization\n",
    "# Convert to lowercase and strip whitespace\n",
    "alexa_df[\"domain\"] = alexa_df[\"domain\"].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd686c9a-38f2-44db-a922-96e69b8132c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Validation\n",
    "# Use tldextract to validate domains (ensure they have a proper TLD)\n",
    "def is_valid_domain(domain):\n",
    "    extracted = tldextract.extract(domain)\n",
    "    return bool(extracted.domain and extracted.suffix)  # Must have a domain and TLD\n",
    "\n",
    "alexa_df = alexa_df[alexa_df[\"domain\"].apply(is_valid_domain)]\n",
    "\n",
    "# Add a label column (benign = 0)\n",
    "alexa_df[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e0b27c-f2e3-4284-bac1-ff1a0738c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Alexa data: (999617, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>google.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>baidu.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank         domain  label\n",
       "0     1     google.com      0\n",
       "1     2    youtube.com      0\n",
       "2     3   facebook.com      0\n",
       "3     4      baidu.com      0\n",
       "4     5  wikipedia.org      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save cleaned data\n",
    "alexa_df.to_csv(\"Data/top-1m_cleaned.csv\", index=False)\n",
    "print(f\"Cleaned Alexa data: {alexa_df.shape}\")\n",
    "alexa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "575788f4-0326-43a2-8348-8723f577c6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhishTank Columns: ['phish_id', 'url', 'phish_detail_url', 'submission_time', 'verified', 'verification_time', 'online', 'target']\n",
      "   phish_id                                    url  \\\n",
      "0   9057481  https://bayareafastrak.org-etcsw.win/   \n",
      "1   9057480  https://bayareafastrak.org-etcsv.win/   \n",
      "2   9057479  https://bayareafastrak.org-etcst.win/   \n",
      "3   9057478  https://bayareafastrak.org-etcsr.win/   \n",
      "4   9057477  https://bayareafastrak.org-etcsq.win/   \n",
      "\n",
      "                                    phish_detail_url  \\\n",
      "0  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "1  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "2  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "3  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "4  http://www.phishtank.com/phish_detail.php?phis...   \n",
      "\n",
      "             submission_time verified          verification_time online target  \n",
      "0  2025-04-11T07:43:02+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "1  2025-04-11T07:42:49+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "2  2025-04-11T07:42:35+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "3  2025-04-11T07:42:22+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n",
      "4  2025-04-11T07:42:07+00:00      yes  2025-04-11T09:12:32+00:00    yes  Other  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the PhishTank dataset\n",
    "# Replace with the actual path to your PhishTank CSV file\n",
    "phishtank_path = os.path.join(base_dir, \"Data/phishtank.csv\")\n",
    "phishtank_df = pd.read_csv(phishtank_path)\n",
    "\n",
    "# Print columns to inspect\n",
    "print(\"PhishTank Columns:\", phishtank_df.columns.tolist())\n",
    "print(phishtank_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f92a3ebc-36d0-42f7-8b0f-1e159ebbf03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract domain names from URLs\n",
    "# Use tldextract to parse URLs and extract domains\n",
    "def extract_domain(url):\n",
    "    try:\n",
    "        extracted = tldextract.extract(url)\n",
    "        domain = f\"{extracted.domain}.{extracted.suffix}\"\n",
    "        return domain if extracted.domain and extracted.suffix else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "phishtank_df[\"domain\"] = phishtank_df[\"url\"].apply(extract_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e7e4f42-225b-4c3c-8c62-150c39dcce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Noise Removal\n",
    "# Remove rows where domain extraction failed\n",
    "phishtank_df = phishtank_df.dropna(subset=[\"domain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03f8ddf-8e32-412f-b0be-e0298ee01073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Deduplication\n",
    "phishtank_df = phishtank_df.drop_duplicates(subset=\"domain\")\n",
    "\n",
    "# Step 5: Normalization\n",
    "# Convert domains to lowercase and strip whitespace\n",
    "phishtank_df[\"domain\"] = phishtank_df[\"domain\"].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50645dc4-bb6e-47ae-a476-a52fcf8fced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Validation\n",
    "# Ensure domains are valid (already handled by tldextract)\n",
    "\n",
    "# Step 7: Add label (malicious = 1)\n",
    "phishtank_df[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a60c1dc-a84c-475c-8c1d-e7e757257a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned PhishTank data: (14138, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org-etcsw.win</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org-etcsv.win</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org-etcst.win</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org-etcsr.win</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org-etcsq.win</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain  label\n",
       "0  org-etcsw.win      1\n",
       "1  org-etcsv.win      1\n",
       "2  org-etcst.win      1\n",
       "3  org-etcsr.win      1\n",
       "4  org-etcsq.win      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select only the columns we need\n",
    "phishtank_df = phishtank_df[[\"domain\", \"label\"]]\n",
    "\n",
    "# Save cleaned data\n",
    "phishtank_df.to_csv(\"Data/phishtank_cleaned.csv\", index=False)\n",
    "print(f\"Cleaned PhishTank data: {phishtank_df.shape}\")\n",
    "phishtank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "088d8cdd-2c0d-4540-bc8f-d2a2a824e883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset: (1013341, 2)\n",
      "label\n",
      "0    999617\n",
      "1     13724\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>google.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baidu.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          domain  label\n",
       "0     google.com      0\n",
       "1    youtube.com      0\n",
       "2   facebook.com      0\n",
       "3      baidu.com      0\n",
       "4  wikipedia.org      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load the cleaned datasets\n",
    "alexa_df = pd.read_csv(\"Data/top-1m_cleaned.csv\")\n",
    "phishtank_df = pd.read_csv(\"Data/phishtank_cleaned.csv\")\n",
    "\n",
    "# Step 2: Combine the datasets\n",
    "combined_df = pd.concat([alexa_df[[\"domain\", \"label\"]], phishtank_df[[\"domain\", \"label\"]]], ignore_index=True)\n",
    "\n",
    "# Step 3: Deduplicate (in case of overlap)\n",
    "combined_df = combined_df.drop_duplicates(subset=\"domain\")\n",
    "\n",
    "# Step 4: Save the combined dataset\n",
    "combined_df.to_csv(\"Data/combined_domains.csv\", index=False)\n",
    "print(f\"Combined dataset: {combined_df.shape}\")\n",
    "print(combined_df[\"label\"].value_counts())  # Check class distribution\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f626bd6-2d37-461e-b893-d8df85adc9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with features: (1013341, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>entropy</th>\n",
       "      <th>num_digits</th>\n",
       "      <th>num_special</th>\n",
       "      <th>vowel_consonant_ratio</th>\n",
       "      <th>domain</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>2.646439</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>google.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>3.095795</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3.022055</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>facebook.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>3.169925</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>baidu.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3.334679</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length   entropy  num_digits  num_special  vowel_consonant_ratio  \\\n",
       "0      10  2.646439           0            1               0.800000   \n",
       "1      11  3.095795           0            1               1.000000   \n",
       "2      12  3.022055           0            1               0.833333   \n",
       "3       9  3.169925           0            1               1.000000   \n",
       "4      13  3.334679           0            1               1.000000   \n",
       "\n",
       "          domain  label  \n",
       "0     google.com      0  \n",
       "1    youtube.com      0  \n",
       "2   facebook.com      0  \n",
       "3      baidu.com      0  \n",
       "4  wikipedia.org      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Function to calculate Shannon entropy of a string\n",
    "def calculate_entropy(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    length = len(text)\n",
    "    freq = Counter(text)\n",
    "    entropy = -sum((count/length) * math.log2(count/length) for count in freq.values())\n",
    "    return entropy\n",
    "\n",
    "# Function to extract lexical features\n",
    "def extract_lexical_features(domain):\n",
    "    # Length of the domain\n",
    "    length = len(domain)\n",
    "    \n",
    "    # Shannon entropy\n",
    "    entropy = calculate_entropy(domain)\n",
    "    \n",
    "    # Number of digits\n",
    "    num_digits = sum(c.isdigit() for c in domain)\n",
    "    \n",
    "    # Number of special characters (non-alphanumeric)\n",
    "    num_special = sum(not c.isalnum() for c in domain)\n",
    "    \n",
    "    # Vowel-to-consonant ratio (excluding dots)\n",
    "    vowels = set(\"aeiou\")\n",
    "    domain_letters = domain.replace(\".\", \"\")\n",
    "    num_vowels = sum(c.lower() in vowels for c in domain_letters)\n",
    "    num_consonants = sum(c.isalpha() and c.lower() not in vowels for c in domain_letters)\n",
    "    vowel_consonant_ratio = num_vowels / (num_consonants + 1e-10)  # Avoid division by zero\n",
    "    \n",
    "    return {\n",
    "        \"length\": length,\n",
    "        \"entropy\": entropy,\n",
    "        \"num_digits\": num_digits,\n",
    "        \"num_special\": num_special,\n",
    "        \"vowel_consonant_ratio\": vowel_consonant_ratio\n",
    "    }\n",
    "\n",
    "# Apply feature extraction to the combined dataset\n",
    "combined_df = pd.read_csv(\"Data/combined_domains.csv\")\n",
    "features = combined_df[\"domain\"].apply(extract_lexical_features)\n",
    "features_df = pd.DataFrame(features.tolist())\n",
    "features_df[\"domain\"] = combined_df[\"domain\"]\n",
    "features_df[\"label\"] = combined_df[\"label\"]\n",
    "\n",
    "# Save the dataset with features\n",
    "features_df.to_csv(\"Data/features_dataset.csv\", index=False)\n",
    "print(f\"Dataset with features: {features_df.shape}\")\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1343dc5f-3020-49b7-b822-3a1eb981d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.853771420394831\n",
      "Precision: 0.056327338722983004\n",
      "Recall: 0.6218579234972678\n",
      "F1-Score: 0.10329803328290468\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92    199924\n",
      "           1       0.06      0.62      0.10      2745\n",
      "\n",
      "    accuracy                           0.85    202669\n",
      "   macro avg       0.53      0.74      0.51    202669\n",
      "weighted avg       0.98      0.85      0.91    202669\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset with features\n",
    "features_df = pd.read_csv(\"Data/features_dataset.csv\")\n",
    "\n",
    "# Prepare features (X) and labels (y)\n",
    "X = features_df[[\"length\", \"entropy\", \"num_digits\", \"num_special\", \"vowel_consonant_ratio\"]]\n",
    "y = features_df[\"label\"]\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5413ae09-49ca-4cfc-8b8f-4cf091ca3a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2a84a2f-4619-4145-b5ec-7f5e66d0f8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign File Columns: ['timestamp', 'FQDN_count', 'subdomain_length', 'upper', 'lower', 'numeric', 'entropy', 'special', 'labels', 'labels_max', 'labels_average', 'longest_word', 'sld', 'len', 'subdomain']\n",
      "                    timestamp  FQDN_count  subdomain_length  upper  lower  \\\n",
      "0  2020-11-20 13:58:38.988039          26                 9      0     10   \n",
      "1  2020-11-20 13:58:39.398160          26                 9      0     10   \n",
      "2  2020-11-20 13:58:39.990691          27                10      0     10   \n",
      "3  2020-11-20 13:58:40.400893          27                10      0     10   \n",
      "4  2020-11-20 13:58:41.636293          24                 7      0     10   \n",
      "\n",
      "   numeric   entropy  special  labels  labels_max  labels_average  \\\n",
      "0       10  2.742338        6       6           7        3.500000   \n",
      "1       10  2.742338        6       6           7        3.500000   \n",
      "2       11  2.767195        6       6           7        3.666667   \n",
      "3       11  2.767195        6       6           7        3.666667   \n",
      "4        8  2.054029        6       6           7        3.166667   \n",
      "\n",
      "  longest_word  sld  len  subdomain  \n",
      "0            2  192   13          1  \n",
      "1            2  192   13          1  \n",
      "2            2  192   14          1  \n",
      "3            2  192   14          1  \n",
      "4            4  224   11          1  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/CIC-BELL-DNS-EXF2021/Attack_heavy_Benign/stateless_features-benign_heavy_1.pcap.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Inspect a mixed file (benign + heavy attack)\u001b[39;00m\n\u001b[32m     10\u001b[39m mixed_heavy_file = \u001b[33m\"\u001b[39m\u001b[33mData/CIC-BELL-DNS-EXF2021/Attack_heavy_Benign/stateless_features-benign_heavy_1.pcap.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m mixed_heavy_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_heavy_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMixed Heavy File Columns:\u001b[39m\u001b[33m\"\u001b[39m, mixed_heavy_df.columns.tolist())\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(mixed_heavy_df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Data/CIC-BELL-DNS-EXF2021/Attack_heavy_Benign/stateless_features-benign_heavy_1.pcap.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inspect a benign file\n",
    "benign_file = \"Data/CIC-BELL-DNS-EXF2021/Benign/stateless_features-benign_1.pcap.csv\"\n",
    "benign_df = pd.read_csv(benign_file)\n",
    "print(\"Benign File Columns:\", benign_df.columns.tolist())\n",
    "print(benign_df.head())\n",
    "\n",
    "# Inspect a mixed file (benign + heavy attack)\n",
    "mixed_heavy_file = \"Data/CIC-BELL-DNS-EXF2021/Attack_heavy_Benign/stateless_features-benign_heavy_1.pcap.csv\"\n",
    "mixed_heavy_df = pd.read_csv(mixed_heavy_file)\n",
    "print(\"\\nMixed Heavy File Columns:\", mixed_heavy_df.columns.tolist())\n",
    "print(mixed_heavy_df.head())\n",
    "\n",
    "# Inspect an attack-only file (heavy attack)\n",
    "attack_heavy_file = \"Data/CIC-BELL-DNS-EXF2021/Attack_heavy_Benign/stateless_features-heavy_audio.pcap.csv\"\n",
    "attack_heavy_df = pd.read_csv(attack_heavy_file)\n",
    "print(\"\\nAttack Heavy File Columns:\", attack_heavy_df.columns.tolist())\n",
    "print(attack_heavy_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8270b0a-0614-45d4-ba82-3ae223d88ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign File Columns: ['timestamp', 'FQDN_count', 'subdomain_length', 'upper', 'lower', 'numeric', 'entropy', 'special', 'labels', 'labels_max', 'labels_average', 'longest_word', 'sld', 'len', 'subdomain']\n",
      "                    timestamp  FQDN_count  subdomain_length  upper  lower  \\\n",
      "0  2020-11-20 13:58:38.988039          26                 9      0     10   \n",
      "1  2020-11-20 13:58:39.398160          26                 9      0     10   \n",
      "2  2020-11-20 13:58:39.990691          27                10      0     10   \n",
      "3  2020-11-20 13:58:40.400893          27                10      0     10   \n",
      "4  2020-11-20 13:58:41.636293          24                 7      0     10   \n",
      "\n",
      "   numeric   entropy  special  labels  labels_max  labels_average  \\\n",
      "0       10  2.742338        6       6           7        3.500000   \n",
      "1       10  2.742338        6       6           7        3.500000   \n",
      "2       11  2.767195        6       6           7        3.666667   \n",
      "3       11  2.767195        6       6           7        3.666667   \n",
      "4        8  2.054029        6       6           7        3.166667   \n",
      "\n",
      "  longest_word  sld  len  subdomain  \n",
      "0            2  192   13          1  \n",
      "1            2  192   13          1  \n",
      "2            2  192   14          1  \n",
      "3            2  192   14          1  \n",
      "4            4  224   11          1  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/CIC-BELL-DNS-EXF2021/Attack_Heavy_Benign/stateless_features-benign_heavy_1.pcap.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Inspect a mixed file (benign + heavy attack) - Corrected path\u001b[39;00m\n\u001b[32m     10\u001b[39m mixed_heavy_file = \u001b[33m\"\u001b[39m\u001b[33mData/CIC-BELL-DNS-EXF2021/Attack_Heavy_Benign/stateless_features-benign_heavy_1.pcap.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m mixed_heavy_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed_heavy_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMixed Heavy File Columns:\u001b[39m\u001b[33m\"\u001b[39m, mixed_heavy_df.columns.tolist())\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(mixed_heavy_df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\START up\\Malicious Domain\\MALICIOUS-DOMAIN-NAME-PREDICTION-SYSTEM-USING-MACHINE-LEARNING\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Data/CIC-BELL-DNS-EXF2021/Attack_Heavy_Benign/stateless_features-benign_heavy_1.pcap.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inspect a benign file (already done, but included for completeness)\n",
    "benign_file = \"Data/CIC-BELL-DNS-EXF2021/Benign/stateless_features-benign_1.pcap.csv\"\n",
    "benign_df = pd.read_csv(benign_file)\n",
    "print(\"Benign File Columns:\", benign_df.columns.tolist())\n",
    "print(benign_df.head())\n",
    "\n",
    "# Inspect a mixed file (benign + heavy attack) - Corrected path\n",
    "mixed_heavy_file = \"Data/CIC-BELL-DNS-EXF2021/Attack_Heavy_Benign/stateless_features-benign_heavy_1.pcap.csv\"\n",
    "mixed_heavy_df = pd.read_csv(mixed_heavy_file)\n",
    "print(\"\\nMixed Heavy File Columns:\", mixed_heavy_df.columns.tolist())\n",
    "print(mixed_heavy_df.head())\n",
    "\n",
    "# Inspect an attack-only file (heavy attack) - Corrected path\n",
    "attack_heavy_file = \"Data/CIC-BELL-DNS-EXF2021/Attack_Heavy_Benign/stateless_features-heavy_audio.pcap.csv\"\n",
    "attack_heavy_df = pd.read_csv(attack_heavy_file)\n",
    "print(\"\\nAttack Heavy File Columns:\", attack_heavy_df.columns.tolist())\n",
    "print(attack_heavy_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173a719-b483-43cb-96e6-c8e3484b44d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
